# llm-judge-eval
Framework and dataset for studying how language models behave as judges. Includes a taxonomy of bias mechanisms, baseline Q/A pairs, and evaluation scripts for testing robustness of LLM judgments.
